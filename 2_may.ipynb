{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2310d320",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d0a0",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?\n",
    "Anomaly detection is the process of identifying data points or patterns that deviate significantly from the normal distribution of data. Its purpose is to detect rare events, fraudulent activities, system malfunctions, or unusual behaviors in various applications like finance, cybersecurity, manufacturing, and healthcare.\n",
    "\n",
    "## Q2. What are the key challenges in anomaly detection?\n",
    "Key challenges include:\n",
    "\n",
    "Class imbalance: Anomalies are rare compared to normal data, making detection challenging.\n",
    "Lack of labeled data: Anomalies are often unlabeled, complicating supervised learning approaches.\n",
    "High-dimensional data: The complexity of high-dimensional datasets makes it difficult to detect anomalies.\n",
    "Dynamic environments: The nature of normal behavior can evolve, causing concept drift.\n",
    "Noise: Distinguishing between noise and actual anomalies can be difficult.\n",
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "Unsupervised anomaly detection identifies anomalies without labeled data. It relies on the assumption that anomalies are rare and differ significantly from normal patterns.\n",
    "Supervised anomaly detection requires labeled data for both normal and anomalous classes. It uses this labeled data to train models to classify future observations.\n",
    "## Q4. What are the main categories of anomaly detection algorithms?\n",
    "The main categories include:\n",
    "\n",
    "Statistical-based methods: Assumes normal data follows a distribution, and anomalies deviate from this distribution.\n",
    "Distance-based methods: Measure the distance between points, with anomalies being far from the majority of points.\n",
    "Density-based methods: Detect anomalies by comparing the density of points; outliers have lower density than normal points.\n",
    "Model-based methods: Create a model for normal behavior and flag deviations from the model as anomalies (e.g., Isolation Forest, Autoencoders).\n",
    "Clustering-based methods: Use clustering algorithms, where anomalies don‚Äôt fit well into any cluster.\n",
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "The main assumptions are:\n",
    "\n",
    "Anomalies are far from other data points.\n",
    "Normal points are densely packed together, while anomalies are isolated.\n",
    "Distance metrics like Euclidean distance can effectively distinguish anomalies from normal points.\n",
    "## Q6. How does the LOF algorithm compute anomaly scores?\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density of data points. It compares the local density of a point to the density of its neighbors. If a point's local density is significantly lower than its neighbors, it is considered an anomaly. The anomaly score is the ratio of the point's density to the average density of its neighbors.\n",
    "\n",
    "## Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "Key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "Number of trees (n_estimators): Determines the size of the ensemble of isolation trees.\n",
    "Subsampling size (max_samples): Number of samples used to build each tree.\n",
    "Contamination: Proportion of anomalies in the dataset (used to set a threshold for decision-making).\n",
    "Max depth: The maximum depth of each isolation tree, affecting how quickly anomalies are isolated.\n",
    "## Q8. If a data point has only 2 neighbors of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "In K-Nearest Neighbors (KNN) anomaly detection, the anomaly score is often based on the distance to the Kth nearest neighbor or the density of neighbors. If the data point has only 2 neighbors of the same class within a radius of 0.5, and K=10, it would have a high anomaly score because the point is isolated from the majority of its neighbors, indicating it is an outlier.\n",
    "\n",
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "In the Isolation Forest algorithm, anomaly scores are based on the average path length of a data point across all trees. Shorter path lengths indicate easier isolation and, thus, a higher likelihood of being an anomaly.\n",
    "\n",
    "For a dataset of 3000 data points, the expected average path length for a data point can be approximated by: \n",
    "ùëê\n",
    "(\n",
    "ùëõ\n",
    ")\n",
    "=\n",
    "2\n",
    "‚ãÖ\n",
    "(\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "ùëõ\n",
    "‚àí\n",
    "1\n",
    ")\n",
    "+\n",
    "0.5772\n",
    ")\n",
    "‚àí\n",
    "2\n",
    "‚ãÖ\n",
    "(\n",
    "ùëõ\n",
    "‚àí\n",
    "1\n",
    ")\n",
    "ùëõ\n",
    "c(n)=2‚ãÖ(log(n‚àí1)+0.5772)‚àí \n",
    "n\n",
    "2‚ãÖ(n‚àí1)\n",
    "‚Äã\n",
    "  For n = 3000, the expected path length is approximately c(3000) ‚âà 10.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc86c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
